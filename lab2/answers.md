# Представленны разборы алгоритмов оптимизации 

## Методы 0 порядка


## Методы 1 порядка

### Метод наискорейшего спуска
Итерационная формула метода спуска:

$$
\overline{x}^{k+1} = \overline{x}^{k} - \lambda^{k}\nabla f(\overline{x}^{k}) \text{ или }\\
\overline{x}^{k+1} = \overline{x}^{k} - \lambda^{k} \frac{\nabla f(\overline{x}^{k})}{\| \nabla f(\overline{x}^{k}) \|} =  \overline{x}^{k} + \lambda^{k}\overline{S}^{k}
$$

$\lambda-$ значительно влияет на траекторию, если 

$\lambda-$ большой, то спуск колебательный процесс 

$\lambda-$ слишком большой то процесс расходится

$\lambda-$ малый слишком много итераций, но траектория будет гладкая 

$\lambda^k=arg(\min\limits_{\lambda}(f(\overline{x}^{k} + \lambda^{k}\overline{S}^{k})))$ - это задача одномерной минимизации для выбора коэффициента релаксации

Если $\lambda$ определяется в результате одномерной минимизации, то градиент в точке очередного приближения будет ортогонален направлению предыдущего спуска: $\nabla{f(\overline{x}^{k+1})} \bot \overline{S}^{k}$

* Процесс может закончиться там, где $\nabla{f(\overline{x})}=\overline{0} \Rightarrow$ нужно проверить не в седловой ли мы точке
* Эффектривность зависит от вида минимизируемой функции. За 1 итерацию будет сходиться функция $f(\overline{x})=x_ {1} ^ {2} + x_ {2} ^ {2}$, а вот функция $f(\overline{x})=x_ {1} ^ {2} + 100x_ {2} ^ {2}$ будет минимизироваться в разы медленнее
* Плохо когда линии уровня вытянуты вдоль оврага
* Предварительное масштабирование может ускороить сходимость 
* Наискорейший спуск хорошо работает вдали точки экстремума и плохо вблизи нее $\Rightarrow$ этот метод используют в связке с другими.  

## Методы 2 порядка
В методах второго порядка при поиске минимума используют ин-
формацию о функции и ее производных до второго порядка включи-
тельно. К этой группе относят метод Ньютона и его модификации

В основе метода лежит квадратичная аппроксимация $f(\overline{x})$ , кото-
рую можно получить, отбрасывая в рядах Тейлора члены третьего и
более высокого порядка:

$$
f(\overline{x}) \approx f(\overline{x}^k) + \nabla^{T}{f(\overline{x}^k)}(\overline{x}-\overline{x}^k)+ \frac{1}{2}(\overline{x}-\overline{x}^k)^{T}\nabla^{2}{f(\overline{x}^k)}(\overline{x}-\overline{x}^k) (1)
$$

где $\nabla^{2}{f(\overline{x}^k)} = H(\overline{x}^k)-$ матрица Гессе, представлающую собой квадратную матрицу из частных производных. 

Направление поиска $\overline{S}^k$ в методе Ньютона определяется следую-
щим образом. Если заменить в выражении (1) $\overline{x}$ на $\overline{x}^{k+1}$ и обозначить $\Delta{\overline{x}^k}=\overline{x}^{k+1}-\overline{x}^k$ то получим 

$$
f(\overline{x}^{k+1}) \approx f(\overline{x}^k) + \nabla^{T}{f(\overline{x}^k)}(\overline{x}-\overline{x}^k)+ \frac{1}{2}(\overline{x}-\overline{x}^k)^{T}\nabla^{2}{f(\overline{x}^k)}(\overline{x}-\overline{x}^k)
$$

Минимум функции $f(\overline{x})$ в направлении $\Delta{\overline{x}^k}$ определяется дифференцированием $f(\overline{x})$ по каждой из компонент $\Delta{\overline{x}}$ и приравниваанием к нулю выражений

$$
\nabla{f(\overline{x}^k)} + \nabla^{2}{f(\overline{x}^k)}\Delta{\overline{x}^k} = \overline{0}
$$

Это приводит к 

$$
\Delta{\overline{x}^k} = -[\nabla^{2}{f(\overline{x}^k)}]^{-1}\nabla{f(\overline{x}^k)}
$$

$$
\overline{x}^{k+1} = \overline{x}^k - [\nabla^{2}{f(\overline{x}^k)}]^{-1}\nabla{f(\overline{x}^k)}
$$

В данном случае и величина шага и направление поиска полностью
определены. Если $f(\overline{x})$ - квадратичная то метод сходится за 1 итерацию. 

Но в общем случае нелинейной функции $f(\overline{x})$ за один шаг мини-
мум не достигается. Поэтому итерационную формулу обычно при-
водят к виду:

$$
\overline{x}^{k+1} = \overline{x}^k - \lambda^{k}\frac{[\nabla^{2}{f(\overline{x}^k)}]^{-1}\nabla{f(\overline{x}^k)}}{\|[\nabla^{2}{f(\overline{x}^k)}]^{-1}\nabla{f(\overline{x}^k)}\|}
$$

$\lambda^{k}$ – параметр длины шага

Направление спуска определяется вектором

$$
\overline{S}^{k}=-H^{-1}(\overline{x}^k)\nabla{f(\overline{x}^k)}
$$

Условие гарантирующее сходимость метода Ньютона в предположении, что функция $f(\overline{x})$ дважды непрерывно дифференцируемая, заключается в том, что матрица $H^{-1}(\overline{x})$ должна быть положительно определенной. 

Иногда на практике сложно вычислить $H^{-1}(\overline{x})$. Тогда делают так. 
Пусть начально приближение достаточно хорошее. Вычисляем матрицу $[\nabla^{2}{f(\overline{x}^{0})}]^{-1}$ и в дальнейшем на всех итерациях используем вместо $[\nabla^{2}{f(\overline{x}^k)}]^{-1}$ используем $[\nabla^{2}{f(\overline{x}^{0})}]^{-1}$ 

Очередное приближение определяется соотношением 

$$
\overline{x}^{k+1} = \overline{x}^k - \lambda^{k}[\nabla^{2}{f(\overline{x}^{0})}]^{-1}\nabla{f(\overline{x}^{k})} = \overline{x}^k - \lambda^{k}H^{-1}(\overline{x}^{0})\nabla{f(\overline{x}^{k})}
$$

Естественно количество итераций возрастает, но в целом процесс может оказаться экономичнее. 

Применение метода Ньютона очень эффективно при условии, что выполняется необходимое и достаточное условие его сходимости. Однако исследование необходимых и достаточных условий сходимости в случае конкретной $f(\overline{x})$ может оказаться весьма затруднительной задачей.
## Методы переменной метрики
